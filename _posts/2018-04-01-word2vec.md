---
layout: post
title: Word2Vec 笔记
categories: NLP
description: Word2Vec 笔记
keywords: NLP
---

## Word2Vec 背景
- W2V是一种将one-hot的词袋向量embedding为低维度稠密向量的算法，embed后的词向量具有语义相关性，语义相近的词，向量间距离也相近。

## 模型
- 模型可以看作有一个隐含层的神经网络
- Input Layer：one-hot vector
- Hidden Layer：线性变换，无激活函数
- Output Layer：维度与Input Layer相等，使用softmax

## 语料样本
- 两种训练模型，CBOW (Continuous Bag-of-Words Model) 和 Skip-gram (Continuous Skip-gram Model)
- CBOW：将一个词所在的上下文中的词作为输入，而那个词本身作为输出。
- Skip-gram：将一个词所在的上下文中的词作为输出，而那个词本身作为输入。

## 优化
- Hierarchical Softmax
  - 借助哈夫曼树使softmax计算量由V减少到logV
- Negative Sampling
  - 2c个词的上下文content(w)，中心词w为正例，通过采样得到N个负例。进行二元逻辑回归得到词向量。
  